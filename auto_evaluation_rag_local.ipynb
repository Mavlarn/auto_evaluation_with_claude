{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79e2bbbc-d038-471b-8193-1572fb5f6647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade --quiet langchain langchain-community chromadb bs4 boto3\n",
    "# %pip install --upgrade --quiet pydantic\n",
    "# %pip install --upgrade --quiet sentence-transformers\n",
    "\n",
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb3d82b-7bcd-4378-8fe6-784519c8bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet trulens_eval\n",
    "# %pip install --upgrade --quiet jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8824bc10-eb43-4f36-929b-9c7c6e6d5199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet ipywidgets>=8.0.6\n",
    "# \"ipython>=8.12.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154308d2-f995-4b32-8e38-8830947691b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.1.14\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/ubuntu/.local/lib/python3.10/site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: trulens-eval\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9078528-14f5-4f41-9489-ed1edc011f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "import json\n",
    "\n",
    "sagemaker_endpoint_name = \"mt-djl-ds-chatglm3-g4dn\"\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        parameters = model_kwargs\n",
    "        if 'temperature' not in parameters:\n",
    "            parameters['temperature'] = 0.01\n",
    "        input = {\"inputs\": prompt, \"parameters\": parameters }\n",
    "        input_str = json.dumps(input).encode('utf-8')\n",
    "        return input_str\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"outputs\"]\n",
    "\n",
    "chatglm_model = SagemakerEndpoint(\n",
    "    endpoint_name=sagemaker_endpoint_name, \n",
    "    region_name=\"us-east-1\", \n",
    "    content_handler=ContentHandler()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025a101e-3613-47a5-a223-8a77e3bd54f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazon Bedrock 是一种基于云的基础设施即服务(IaaS)工具，旨在为开发人员和运维团队提供一种可靠、可扩展的基础设施管理方式。它提供了一个统一的管理界面，让用户可以轻松管理 AWS 资源，包括计算、存储、网络、数据库等。Bedrock 的目标是简化 AWS 的基础设施管理，使用户能够更专注于应用程序的开发和部署，而不必担心底层基础设施的复杂性。\\n\\nBedrock 提供了一些核心功能，包括：\\n\\n1. 统一管理界面：Bedrock 提供了一个统一的控制台，让用户可以轻松管理 AWS 资源。\\n2. 自动化：Bedrock 提供了自动化工具，可以帮助用户自动化各种基础设施管理任务，例如创建、更新和删除资源。\\n3. 可扩展性：Bedrock 旨在提供一种可扩展的基础设施管理方式，可以适应各种规模和需求的场景。\\n4. 安全：Bedrock 提供了各种安全工具和功能，帮助用户保护基础设施和数据安全。\\n\\n总的来说，Amazon Bedrock 是一种强大的基础设施管理工具，可以帮助用户更轻松地管理 AWS 资源，提高效率和可靠性。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatglm_model.invoke(\"Amazon Bedrock是什么?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02f337a-12dd-4f94-9885-631c48394eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='moka-ai/m3e-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f2546-d2ba-4ce9-8eb4-33172664d0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c496afba-f571-4bbb-9c7a-705a2cd9a68d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\"https://huggingface.co/blog/zh/moe\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=\"./chroma_db_500chunk/\")\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e28da",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1030f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "vectorstore = Chroma(embedding_function=embeddings, persist_directory=\"./chroma_db_500chunk/\")\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e61bed4-97bf-484e-91c9-533890178454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"MOE有什么优势?\",\n",
    "    \"为什么MOE的训练和推理的速度更快?\",\n",
    "    \"简单总结MoE模型的特点?\",\n",
    "    \"Shazeer 将 MoE 应用在了哪个应用领域?\",\n",
    "    \"MoE模型的训练过程中，如何保证令牌不会总是被发送给少数几个受欢迎的专家?\",\n",
    "    \"MoE模型训练时，怎么保证token的负载均衡?\",\n",
    "    \"GShard如何保证MoE模型的负载均衡?\",\n",
    "    \"MoE的专家数量达到多少以后，效率就会显著降低?\",\n",
    "    \"怎么将MoE模型的推理阶段更加高效?\",\n",
    "    \"在微调MoE这样的稀疏模型时，怎么防止出现过拟合?\",\n",
    "    \"模型的并行有几种形式?\",\n",
    "    \"部署 MoE 模型有哪些技术?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b5abbac-b366-4e56-9472-d99b9402c752",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Switch Transformers paper](https://arxiv.org/abs/2101.03961) 论文中的 MoE layer\n",
      "\n",
      "总结来说，在混合专家模型 (MoE) 中，我们将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个门控网络和若干数量的专家。\n",
      "尽管混合专家模型 (MoE) 提供了若干显著优势，例如更高效的预训练和与稠密模型相比更快的推理速度，但它们也伴随着一些挑战:\n",
      "了解了 MoE 的基本概念后，让我们进一步探索推动这类模型发展的研究。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t混合专家模型简史\n",
      "\t\n",
      "\n",
      "混合专家模型 (MoE) 的理念起源于 1991 年的论文 Adaptive Mixture of Local Experts。这个概念与集成学习方法相似，旨在为由多个单独网络组成的系统建立一个监管机制。在这种系统中，每个网络 (被称为“专家”) 处理训练样本的不同子集，专注于输入空间的特定区域。那么，如何选择哪个专家来处理特定的输入呢？这就是门控网络发挥作用的地方，它决定了分配给每个专家的权重。在训练过程中，这些专家和门控网络都同时接受训练，以优化它们的性能和决策能力。\n",
      "在 2010 至 2015 年间，两个独立的研究领域为混合专家模型 (MoE) 的后续发展做出了显著贡献:\n"
     ]
    }
   ],
   "source": [
    "results = vectorstore.similarity_search(questions[0], k=2)\n",
    "for r in results:\n",
    "    print(r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759c18d-4fe1-41bf-983a-366a4b64b545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5f7f1e1-7e7e-4638-9f76-455e13cb93f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"请基于以下的3引号里面的内容，简洁、准确的回答最后的问题，并使用中文。如果你无法从已知内容获取答案，就直接返回'根据已知信息无法回答该问题。'，不要编造答案。\n",
    "\n",
    "已知内容:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "问题: {question}\n",
    "答案:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7da09ac1-7b9e-4ca1-bbcd-97cf5e7eccad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"请基于以下的3引号里面的内容，简洁、准确的回答最后的问题，并使用中文。如果你无法从已知内容获取答案，就直接返回'根据已知信息无法回答该问题。'，不要编造答案。\\n\\n已知内容:\\n```\\nxcccccc\\n```\\n\\n问题: qqqqq\\n答案:\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(question='qqqqq', context= 'xcccccc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1e0ef-3322-4e30-abe5-f44259411794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c4293f8-c8cc-4ada-9540-ebf3bedeceed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦑 Tru initialized with db url sqlite:///default.sqlite .\n",
      "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "# initialize evaluation db\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0c1984b-a058-424f-b17a-0002bf9cae2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trulens_eval.tru_custom_app import instrument\n",
    "\n",
    "class RAG_App:\n",
    "    @instrument\n",
    "    def retrieve(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        results = vectorstore.similarity_search(query, k=5)\n",
    "        docs = [doc.page_content for doc in results]\n",
    "        return docs\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, query: str, context: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from context.\n",
    "        \"\"\"\n",
    "        context_str = \"\\n\\n\".join(context)\n",
    "        p = prompt_template.format(question=query, context=context_str)\n",
    "        return chatglm_model(p)\n",
    "\n",
    "    @instrument\n",
    "    def query(self, query: str) -> str:\n",
    "        context_slist = self.retrieve(query)\n",
    "        completion = self.generate_completion(query, context_slist)\n",
    "        return completion\n",
    "\n",
    "rag = RAG_App()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e21406c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Bedrock\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "class Claude3_Bedrock(Bedrock):\n",
    "    def _create_chat_completion(\n",
    "        self,\n",
    "        prompt: Optional[str] = None,\n",
    "        messages: Optional[Sequence[Dict]] = None,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        assert self.endpoint is not None\n",
    "\n",
    "        import json\n",
    "\n",
    "        print(\"prompt\", prompt)\n",
    "        print(\"messages\", messages)\n",
    "\n",
    "        if messages:\n",
    "            pass\n",
    "        elif prompt:\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }]\n",
    "        else:\n",
    "            raise ValueError(\"Either 'messages' or 'prompt' must be supplied.\")\n",
    "        \n",
    "        body = json.dumps(\n",
    "            {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 2048,\n",
    "                \"temperature\": 0.01,\n",
    "                \"messages\": messages\n",
    "            }\n",
    "        )\n",
    "\n",
    "        accept = \"application/json\"\n",
    "        content_type = \"application/json\"\n",
    "\n",
    "        response = self.endpoint.client.invoke_model(\n",
    "            body=body, modelId=self.model_id\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response.get('body').read()).get('content')[0]['text']\n",
    "\n",
    "        return response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68117636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f2d5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fclaude = Claude3_Bedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6a47aa4-0fc8-4964-8eb8-631a082a70b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Groundedness, input source will be set to __record__.app.retrieve.rets.collect() .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input question will be set to __record__.app.retrieve.args.query .\n",
      "✅ In Context Relevance, input context will be set to __record__.app.retrieve.rets.collect() .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Feedback, Select, Bedrock\n",
    "\n",
    "from trulens_eval.feedback import Groundedness\n",
    "import numpy as np\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=fclaude)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(fclaude.relevance_with_cot_reasons, name = \"Answer Relevance\").on_input_output()\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(fclaude.qs_relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "    .on(Select.RecordCalls.retrieve.args.query)\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d5a56b0-e568-42be-b75b-f58a4ae315f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trulens_eval import TruCustomApp\n",
    "tru_rag = TruCustomApp(rag, app_id='ChatGLM_500Chunk_Sonnet', feedbacks=[f_groundedness, f_qa_relevance, f_context_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b55414-1752-4759-aef0-1facd49b9ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "529a9310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201ee614be9b4afca13cab83014f4873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tru_rag as recording:\n",
    "    rag.query(questions[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c2fc775-5bef-4602-b8df-0fc235b67c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with tru_rag as recording:\n",
    "#     for question in questions:\n",
    "#         rag.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e3ef238-29f9-4d7a-8be6-babe89dfe0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ChatGLM_500Chunk</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Context Relevance  Groundedness  Answer Relevance  latency  \\\n",
       "app_id                                                                         \n",
       "ChatGLM_500Chunk                1.0           1.0               1.0      2.0   \n",
       "\n",
       "                  total_cost  \n",
       "app_id                        \n",
       "ChatGLM_500Chunk         0.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b2255-dd8a-4c14-b944-b015a0346989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tru_recorder as recording:\n",
    "#     for question in questions:\n",
    "#         rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd1d632f-2f5a-467a-9070-f8f1047b4f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746760eda58b45839a5b0850548d8381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://172.31.20.134:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()\n",
    "# tru.stop_dashboard() # stop if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5322e41-23f2-4ca4-96b3-be5d45199e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bf935-10c9-441e-9dad-cd7064c5d181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babd30c-5652-464c-9d63-c4e57e4aaa81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23953bd2-0a2c-49f0-b586-5c784ad1cbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75d44bd8-7db3-468e-b017-e59135d77082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import BedrockChat\n",
    "import boto3\n",
    "\n",
    "session = boto3.Session()\n",
    "bedrock_client = session.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "model_kwargs = {\n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0.01,\n",
    "    # \"top_k\": 250,\n",
    "    # \"top_p\": 1,\n",
    "}\n",
    "\n",
    "claude3_llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    client=bedrock_client,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bda02413-33ce-4c0a-97e8-b4a4ccf94e61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='星际穿越是一部非常出色的科幻电影,从多个方面都值得一看:\\n\\n1. 科学性强 - 电影中涉及到的相对论、黑洞、高维度等概念都有一定的科学依据,导演与著名物理学家科普作家合作,努力让科学内容尽可能贴近现有理论。\\n\\n2. 视觉效果震撼 - 太空旅行、异次元空间等场景的视觉呈现非常梦幻而宏大,给人深刻的视觉冲击。\\n\\n3. 剧情设置巧妙 - 人类面临资源枯竭的未来,科学家们孤注一掷寻找新家园的故事情节紧凑且富有张力。\\n\\n4. 探讨哲理命题 - 影片蕴含了对爱、家庭、牺牲、人性等命题的思考,给人以启迪。\\n\\n5. 演员阵容出色 - 马修·麦康纳、安妮·海瑟薇等主演都有精彩的演绎。\\n\\n总的来说,这部电影在视觉体验和思想内涵上都达到了很高的水准,是一部值得反复观赏的佳作。你看过这部影片吗?有何其他感想?', response_metadata={'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0', 'usage': {'prompt_tokens': 22, 'completion_tokens': 396, 'total_tokens': 418}}, id='run-8dbbbbe1-955d-4779-bcdc-4ecc687d0311-0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"星际穿越这部电影怎么样?\")\n",
    "]\n",
    "claude3_llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2a64845-a49c-48ca-8e15-9e1ab88333c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='星际穿越是一部非常出色的科幻电影,从多个方面都值得一看:\\n\\n1. 科学性强 - 电影中涉及到的相对论、黑洞、高维度等概念都有一定的科学依据,导演与著名物理学家科普作家合作,努力让科学内容尽可能贴近现有理论。\\n\\n2. 视觉效果震撼 - 太空旅行、异次元空间等场景的视觉呈现非常梦幻而宏大,给人深刻的视觉冲击。\\n\\n3. 剧情设置巧妙 - 人类为了生存不得不去寻找新家园的故事情节引人入胜,加上亲情的元素也很打动人心。\\n\\n4. 哲理性强 - 影片蕴含了对时间、爱、生命等哲学命题的思考,给人以启迪。\\n\\n5. 演员阵容强大 - 马修·麦康纳、安妮·海瑟薇等主演都演技出众。\\n\\n总的来说,这部电影在娱乐性与思考性之间取得了很好的平衡,是一部值得细细品味的佳作。你看过这部影片吗?有何其他感想?', response_metadata={'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0', 'usage': {'prompt_tokens': 22, 'completion_tokens': 381, 'total_tokens': 403}}, id='run-25cd6003-5bf3-4644-aa31-98bb0399394f-0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 与上面的调用一样，后面会使用message API调用\n",
    "claude3_llm.invoke(\"星际穿越这部电影怎么样?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb999383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7d5e794-9438-409e-bcbe-29703dcae450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d92ad79c-1c77-429b-9dec-f3831f92fc41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据已知信息，混合专家模型（MoE）的优势包括：1. 更高效的预训练，2. 与稠密模型相比，推理速度更快。'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.query(\"MOE有什么优势?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c34b261d-fb6c-40c4-99b6-374ba17a2ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'根据 Switch Transformers 论文中的描述，混合专家模型 (MoE) 通过将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个门控网络和若干数量的专家。这种替换可以提高训练和推理速度。具体来说，由于 MoE 模型只使用其中的一部分参数，所以在推理过程中只需要加载部分参数到内存中，从而降低了内存需求。此外，由于 MoE 模型中的专家数量相对较少，因此训练和推理过程中计算量更小，从而提高了速度。'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"为什么MOE的训练和推理的速度更快?\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "587a7669-caa8-4a0a-9edf-ae7915fde680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MoE模型是一种混合专家模型的简写，它将传统Transformer模型中的每个前馈网络（FFN）层替换为MoE层，其中MoE层由两个核心部分组成：一个门控网络和若干数量的专家。MoE模型具有以下特点：1. 训练效率高，能够实现更高效的计算预训练；2. 与稠密模型相比，推理速度更快；3. 具有较好的泛化能力，但在微调阶段面临泛化能力不足的问题；4. 参数需求较大，推理过程中只使用其中一部分，需要将所有参数加载到内存中，因此对内存的需求较高。'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"简单总结MoE模型的特点\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36f68792-e3b0-4f43-80b0-4d168b9b188a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Shazeer 将 MoE 应用于自然语言处理（NLP）领域。'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Shazeer将 MoE 应用在了哪个应用领域？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c77ba28-afd4-46ee-a79f-04685478c473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ST-MoE 论文中显示了哪些令牌组被发送给了哪个专家的表格。根据文中所述，混合专家模型 (MoE) 中的令牌会根据专家的权重比例被分配，以保证所有专家接收到大致相等数量的训练样本，从而平衡专家之间的选择。此外，辅助损失可以被用来鼓励给予所有专家相同的重要性，以避免令牌总是被发送给少数几个受欢迎的专家。'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"MoE模型的训练过程中，如何保证令牌不会总是被发送给少数几个受欢迎的专家？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a528c839-7216-4c16-913c-3c4ea69f089a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据所提供的信息，MoE模型在训练时通过令牌路由和负载均衡机制来保证token的负载均衡。具体来说，该模型将令牌发送到拥有所需专家的节点，每个节点处理不同批次的训练样本。这种设计可以确保每个专家接收到不同数量的令牌，从而实现token的负载均衡。此外，该模型还采用了容量因子来提高模型性能，但这也意味着更高的通信成本和对保存激活值的显存的需求。在设备通信带宽有限的情况下，选择较小的容量因子可能是更佳的策略。'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"MoE模型训练时，怎么保证token的负载均衡？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a942a34a-e35a-4d37-9b23-1c029a2d2e25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GShard 保证 MoE 模型的负载均衡主要通过以下几个方面：\\n1. 随机路由：在 Top-2 设置中，我们始终选择排名最高的专家，但第二个专家是根据其权重比例随机选择的。\\n2. 专家容量：可以设定一个阈值，定义一个专家能处理多少令牌。如果两个专家的容量都达到上限，令牌就会溢出，并通过残差连接传递到下一层，或在某些情况下被完全丢弃。\\n3. 辅助损失：为了鼓励给予所有专家相同的重要性，引入了一个辅助损失，确保所有专家接收到大致相等数量的训练样本，从而平衡了专家之间的选择。\\n通过这些措施，GShard 能够实现 MoE 模型的负载均衡，从而提高训练效率。'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"GShard如何保证MoE模型的负载均衡？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11db4b19-d06d-426a-b950-5221297f50ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据所提供的信息，混合专家模型（MoE）的专家数量达到一定程度后，效率会显著降低。具体来说，当专家数量超过一定程度（约为1000-2000）时，训练和推理速度会明显变慢。因此，在选择专家数量时，需要权衡模型性能和计算资源的使用。'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"MoE的专家数量达到多少以后，效率就会显著降低？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aaf63bca-d75c-4249-9f88-f8f633ddd74d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这是一个比较复杂的问题，因为机器学习模型的效率受到很多因素的影响，包括数据集大小、模型结构、超参数选择等等。同时，专家数量也不是一个固定的值，而是需要根据具体问题进行调整。\\n\\n一般来说，当专家数量达到一定程度时，模型的效率可能会出现下降趋势。这是因为随着专家数量的增加，模型的复杂度也会增加，导致模型训练时间增加，同时过拟合的风险也会增加。但是，具体多少数量的专家会使得效率显著降低，需要根据具体问题和数据集情况进行实验和分析。\\n\\n另外，还有一些技巧可以帮助提高模型效率，例如正则化、早停、Dropout等。这些技巧可以在一定程度上减少过拟合现象，提高模型的泛化能力，同时也可以降低模型的复杂度，提高训练效率。'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatglm_model(\"MoE的专家数量达到多少以后，效率就会显著降低？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f970cab2-32b3-416c-b497-dae02450efc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据ST-MoE论文，微调稀疏混合专家模型（MoE）时，可以通过以下方法防止过拟合：\\n\\n1. 降低学习率和调大批量可以提升稀疏模型微调质量。\\n2. 尝试冻结所有非专家层的权重，实践中，这会导致性能大幅下降，但这符合预期，因为混合专家模型（MoE）层占据了网络的主要部分。可以尝试相反的方法：仅冻结MoE层的参数。实验结果显示，这种方法几乎与更新所有参数的效果相当。这种做法可以加速微调过程，并降低显存需求。\\n3. 考虑特别的微调超参数设置，例如，稀疏模型往往更适合使用较小的批量大小和较高的学习率，这样可以获得更好的训练效果。'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"在微调MoE这样的稀疏模型时，怎么防止出现过拟合？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4619ca89-0c53-46af-a761-3f87a67eefa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'将MoE模型的推理阶段更加高效的方法包括：\\n\\n1. 优化门控网络和专家的权重训练过程，以提高模型在微调阶段的泛化能力。\\n2. 采用更高效的计算方法，例如将稀疏混合专家模型 (SMoE) 蒸馏回具有更少实际参数但相似等价参数量的稠密模型。\\n3. 探索合并专家模型的技术及其对推理时间的影响。\\n4. 尝试对Mix'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"怎么将MoE模型的推理阶段更加高效？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52b3e27f-6fed-492f-985f-35c3932964ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'问题: 模型的并行有几种形式？\\n答案: 数据并行、模型并行、模型和数据并行、专家并行。'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"模型的并行有几种形式？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aaead7cf-7f50-4ea2-85a7-d807038eeb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'部署 MoE 模型有以下几种技术：\\n\\n1. 预先蒸馏 (Pre-distillation)：将 MoE 模型蒸馏回其对应的稠密模型，以保留稀疏性带来的性能提升，同时使得在推理中使用更小型的模型成为可能。\\n\\n2. 任务级别路由 (Task-level routing)：将整个句子或任务直接路由到一个专家，以提取出一个用于服务的子网络，有助于简化模型的结构。\\n\\n3. 专家网络聚合 (Expert network aggregation)：通过合并各个专家的权重，在推理时减少了所需的参数数量，在不显著牺牲性能的情况下降低模型的复杂度。'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"部署 MoE 模型有哪些技术?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f69fbd-4120-460a-87ce-115f45c4e0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87240bb2-f904-415f-acf0-4b90af15b786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "11bec224-dcab-46db-87eb-63bb10ad9d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "75e7102f-b99b-47bc-93b9-9360fcd200fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b5720-d341-4c3b-b768-5739fb859539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e91e01-c178-45ab-b3a1-a65e148accc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa53b4-3c34-462a-89f2-5dc91fa5250d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe95e5-a506-4aed-bbab-60982914a9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b04c2-e808-470d-b2fc-8005cba01497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5243026f-1408-433b-9c45-8cc727f957d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec36720-dc00-434b-9d58-3d33134b99e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c882a-1714-423e-a430-970b34eb38f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "053fc7df-0d25-44b1-92fc-f9abbaecbca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f0385-8341-4c7b-947e-f5021ad49292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b2741-ce7e-4a02-b160-ad9360f8e7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686fdb1-7715-4dd6-9dc0-c3d016d7c134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
