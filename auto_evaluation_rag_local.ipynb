{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e2bbbc-d038-471b-8193-1572fb5f6647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade --quiet langchain langchain-community chromadb bs4 boto3\n",
    "# %pip install --upgrade --quiet pydantic\n",
    "%pip install --upgrade --quiet sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddb3d82b-7bcd-4378-8fe6-784519c8bf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet trulens_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8824bc10-eb43-4f36-929b-9c7c6e6d5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet \"ipython>=8.12.0\" \"ipywidgets>=8.0.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154308d2-f995-4b32-8e38-8830947691b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.1.14\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/ec2-user/miniconda3/envs/llm_notebook_env/lib/python3.11/site-packages\n",
      "Requires: aiohttp, dataclasses-json, jsonpatch, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff9de33-f3dc-4eeb-8d34-8dc68a1ca745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9078528-14f5-4f41-9489-ed1edc011f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "import json\n",
    "\n",
    "sagemaker_endpoint_name = \"mt-djl-ds-chatglm3-g4dn\"\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        parameters = model_kwargs\n",
    "        if 'temperature' not in parameters:\n",
    "            parameters['temperature'] = 0.01\n",
    "        input = {\"inputs\": prompt, \"parameters\": parameters }\n",
    "        input_str = json.dumps(input).encode('utf-8')\n",
    "        return input_str\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"outputs\"]\n",
    "\n",
    "chatglm_model = SagemakerEndpoint(\n",
    "    endpoint_name=sagemaker_endpoint_name, \n",
    "    region_name=\"us-east-1\", \n",
    "    content_handler=ContentHandler()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025a101e-3613-47a5-a223-8a77e3bd54f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazon Bedrock 是一种基于云的基础设施即服务(IaaS)工具，旨在为开发人员和运维团队提供一种可靠、可扩展的基础设施管理方式。它提供了一个统一的管理界面，让用户可以轻松管理 AWS 资源，包括计算实例、存储、数据库、网络等等。\\n\\nAmazon Bedrock 的设计目标是简化 AWS 基础设施的管理，同时提供高可用性、可靠性和性能。它使用自动化工具和脚本来管理基础设施，并使用机器学习来自动检测和预测潜在的问题。此外，Amazon Bedrock 还提供了一种名为“基础设施即代码”(Infrastructure as Code)的方式，让用户可以将基础设施配置和代码化，以便更好地管理和版本控制。\\n\\n总的来说，Amazon Bedrock 提供了一种简单、高效、可靠的方式，帮助用户管理 AWS 基础设施，并提高了运维效率。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatglm_model.invoke(\"Amazon Bedrock是什么?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02f337a-12dd-4f94-9885-631c48394eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='aspire/acge_text_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f2546-d2ba-4ce9-8eb4-33172664d0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c496afba-f571-4bbb-9c7a-705a2cd9a68d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\"https://huggingface.co/blog/zh/moe\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=\"./chroma_db_500chunk/\")\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e61bed4-97bf-484e-91c9-533890178454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"MOE有什么优势?\",\n",
    "    \"为什么MOE的训练和推理的速度更快?\",\n",
    "    \"简单总结MoE模型的特点?\",\n",
    "    \"Shazeer 将 MoE 应用在了哪个应用领域?\",\n",
    "    \"MoE模型的训练过程中，如何保证令牌不会总是被发送给少数几个受欢迎的专家?\",\n",
    "    \"MoE模型训练时，怎么保证token的负载均衡?\",\n",
    "    \"GShard如何保证MoE模型的负载均衡?\",\n",
    "    \"MoE的专家数量达到多少以后，效率就会显著降低?\",\n",
    "    \"怎么将MoE模型的推理阶段更加高效?\",\n",
    "    \"在微调MoE这样的稀疏模型时，怎么防止出现过拟合?\",\n",
    "    \"模型的并行有几种形式?\",\n",
    "    \"部署 MoE 模型有哪些技术?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5abbac-b366-4e56-9472-d99b9402c752",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='组件专家: 在传统的 MoE 设置中，整个系统由一个门控网络和多个专家组成。在支持向量机 (SVMs) 、高斯过程和其他方法的研究中，MoE 通常被视为整个模型的一部分。然而，Eigen、Ranzato 和 Ilya 的研究 探索了将 MoE 作为更深层网络的一个组件。这种方法允许将 MoE 嵌入到多层网络中的某一层，使得模型既大又高效。\\n条件计算: 传统的神经网络通过每一层处理所有输入数据。在这一时期，Yoshua Bengio 等研究人员开始探索基于输入令牌动态激活或停用网络组件的方法。', metadata={'description': 'We’re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/zh/moe', 'title': '混合专家模型（MoE）详解'}),\n",
       " Document(page_content='组件专家: 在传统的 MoE 设置中，整个系统由一个门控网络和多个专家组成。在支持向量机 (SVMs) 、高斯过程和其他方法的研究中，MoE 通常被视为整个模型的一部分。然而，Eigen、Ranzato 和 Ilya 的研究 探索了将 MoE 作为更深层网络的一个组件。这种方法允许将 MoE 嵌入到多层网络中的某一层，使得模型既大又高效。\\n条件计算: 传统的神经网络通过每一层处理所有输入数据。在这一时期，Yoshua Bengio 等研究人员开始探索基于输入令牌动态激活或停用网络组件的方法。', metadata={'description': 'We’re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/zh/moe', 'title': '混合专家模型（MoE）详解'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = vectorstore.similarity_search(questions[0], k=2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96713cb7-01ca-4009-a340-6d72907b84fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759c18d-4fe1-41bf-983a-366a4b64b545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee46df0-749a-4ecb-b936-69a0e0226bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5f7f1e1-7e7e-4638-9f76-455e13cb93f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"请基于以下的3引号里面的内容，简洁、准确的回答最后的问题，并使用中文。如果你无法从已知内容获取答案，就直接返回'根据已知信息无法回答该问题。'，不要编造答案。\n",
    "\n",
    "已知内容:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "问题: {question}\n",
    "答案:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7da09ac1-7b9e-4ca1-bbcd-97cf5e7eccad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"请基于以下的3引号里面的内容，简洁、准确的回答最后的问题，并使用中文。如果你无法从已知内容获取答案，就直接返回'根据已知信息无法回答该问题。'，不要编造答案。\\n\\n已知内容:\\n```\\nxcccccc\\n```\\n\\n问题: qqqqq\\n答案:\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(question='qqqqq', context= 'xcccccc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1e0ef-3322-4e30-abe5-f44259411794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c4293f8-c8cc-4ada-9540-ebf3bedeceed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦑 Tru initialized with db url sqlite:///default.sqlite .\n",
      "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "# initialize evaluation db\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0c1984b-a058-424f-b17a-0002bf9cae2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trulens_eval.tru_custom_app import instrument\n",
    "\n",
    "class RAG_App:\n",
    "    @instrument\n",
    "    def retrieve(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        results = vectorstore.similarity_search(query, k=5)\n",
    "        docs = [doc.page_content for doc in results]\n",
    "        return docs\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, query: str, context: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from context.\n",
    "        \"\"\"\n",
    "        context_str = \"\\n\\n\".join(context)\n",
    "        p = prompt_template.format(question=query, context=context_str)\n",
    "        return chatglm_model(p)\n",
    "\n",
    "    @instrument\n",
    "    def query(self, query: str) -> str:\n",
    "        context_slist = self.retrieve(query)\n",
    "        completion = self.generate_completion(query, context_slist)\n",
    "        return completion\n",
    "\n",
    "rag = RAG_App()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6a47aa4-0fc8-4964-8eb8-631a082a70b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Groundedness, input source will be set to __record__.app.retrieve.rets.collect() .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input question will be set to __record__.app.retrieve.args.query .\n",
      "✅ In Context Relevance, input context will be set to __record__.app.retrieve.rets.collect() .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Feedback, Select, Bedrock\n",
    "from trulens_eval.feedback import Groundedness\n",
    "import numpy as np\n",
    "\n",
    "fclaude = Bedrock(model_id = \"anthropic.claude-v2\", region_name='us-east-1')\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=fclaude)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(fclaude.relevance_with_cot_reasons, name = \"Answer Relevance\").on_input_output()\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(fclaude.qs_relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "    .on(Select.RecordCalls.retrieve.args.query)\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d5a56b0-e568-42be-b75b-f58a4ae315f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trulens_eval import TruCustomApp\n",
    "tru_rag = TruCustomApp(rag, app_id='ChatGLM_500Chunk', feedbacks=[f_groundedness, f_qa_relevance, f_context_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84b55414-1752-4759-aef0-1facd49b9ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c2fc775-5bef-4602-b8df-0fc235b67c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniconda3/envs/llm_notebook_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "App ChatGLM_500Chunk was not present in database. Adding it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6665d10acfba4928b93aa6078e177e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576dc429cfe548788daae4968f0ab797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniconda3/envs/llm_notebook_env/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/ec2-user/miniconda3/envs/llm_notebook_env/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5190cf769446ab9f4f67bd4ebaf26f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13949dc42964703b4a7afb34eae6715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d95988158c14e8c951cb352341640c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb4cb08e88c4c0f82bfe438681cc5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6983a79aac8942b790b8fce209fda8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84245117594e41b6a17ddaf7d554546a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82af94dd61a46d08d7ee6ff7bf7e517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3683951b0442a3ae4722bfefeed912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple valid rating values found in the string: Statement Sentence: MoE模型的多任务指令微调性能优于单任务微调, Supporting Evidence: 当研究者们对 MoE 和对应性能相当的 T5 模型进行微调时,他们发现 T5 的对应模型表现更为出色。然而,当研究者们对 Flan T5 (一种 T5 的指令优化版本) 的 MoE 版本进行微调时,MoE 的性能显著提升。更值得注意的是,Flan-MoE 相比原始 MoE 的性能提升幅度超过了 Flan T5 相对于原始 T5 的提升,这意味着 MoE 模型可能从指令式微调中获益更多,甚至超过了稠密模型。此外,MoE 在多任务学习中表现更佳。与之前关闭辅助损失函数的做法相反,实际上这种损失函数可以帮助防止过拟合。, Score: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e2b5fbd85341658e7b74feb626014e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678c6738479e4b1b89fd592c96271b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tru_rag as recording:\n",
    "    for question in questions:\n",
    "        rag.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e3ef238-29f9-4d7a-8be6-babe89dfe0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ChatGLM_500Chunk</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.9025</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Context Relevance  Answer Relevance  Groundedness  latency  \\\n",
       "app_id                                                                         \n",
       "ChatGLM_500Chunk                0.8          0.981818        0.9025      7.5   \n",
       "\n",
       "                  total_cost  \n",
       "app_id                        \n",
       "ChatGLM_500Chunk         0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b2255-dd8a-4c14-b944-b015a0346989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tru_recorder as recording:\n",
    "#     for question in questions:\n",
    "#         rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd1d632f-2f5a-467a-9070-f8f1047b4f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0722da8bb347618401ebf4b6e8901e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://172.31.2.24:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()\n",
    "# tru.stop_dashboard() # stop if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5322e41-23f2-4ca4-96b3-be5d45199e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bf935-10c9-441e-9dad-cd7064c5d181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babd30c-5652-464c-9d63-c4e57e4aaa81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23953bd2-0a2c-49f0-b586-5c784ad1cbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75d44bd8-7db3-468e-b017-e59135d77082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda02413-33ce-4c0a-97e8-b4a4ccf94e61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "session = boto3.Session()\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region_name = boto3.session.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "bedrock_client = session.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "inference_modifier = {\n",
    "    \"max_tokens_to_sample\": 4096,\n",
    "    \"temperature\": 0.25,\n",
    "    \"top_k\": 150,\n",
    "    \"top_p\": 1,\n",
    "}\n",
    "modelId = 'anthropic.claude-v2'\n",
    "claude_llm = Bedrock(\n",
    "    model_id=modelId,\n",
    "    client=bedrock_client,\n",
    "    model_kwargs=inference_modifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2a64845-a49c-48ca-8e15-9e1ab88333c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e68a230f-9b24-47b2-8a3d-4956d557450e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/mt_python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' GCP 的 Low C02 是指 Google Cloud Platform 提供的一种服务器配置,其特点是使用可再生能源,并且碳排放量较低。\\n\\n具体来说,Low C02 服务器使用的电力中至少包含了75%的可再生能源,如风能、太阳能等。相比普通服务器,这种服务器的碳排放量至少降低了30%。\\n\\n采用 Low C02 服务器可以帮助企业和组织降低其云计算相关的碳排放和环境影响。这对于一些注重可持续发展、环境保护的客户来说具有吸引力。\\n\\n使用 Low C02 服务器不会影响服务器的性能和功能,但会略微增加成本。总体来说,这是 GCP 提供的一个让云计算更环保的选择。企业可以根据自己的需求和可持续发展策略来决定是否采用这种服务器配置。'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claude_llm.invoke(\"GCP的Low C02 是什么意思？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7d5e794-9438-409e-bcbe-29703dcae450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d92ad79c-1c77-429b-9dec-f3831f92fc41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MOE（混合专家模型）相对于稠密模型的优势在于预训练速度更快和推理速度更快。同时，MOE需要的显存较少，且在微调方面存在诸多挑战。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"MOE有什么优势?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c34b261d-fb6c-40c4-99b6-374ba17a2ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'根据 Switch Transformers 论文中的描述，混合专家模型 (MoE) 通过将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个门控网络和若干数量的专家。这种替换可以提高训练和推理速度。具体来说，由于 MoE 模型只使用其中的一部分参数，所以在推理过程中只需要加载部分参数到内存中，从而降低了内存需求。此外，由于 MoE 模型中的专家数量相对较少，因此训练和推理过程中计算量更小，从而提高了速度。'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"为什么MOE的训练和推理的速度更快?\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "587a7669-caa8-4a0a-9edf-ae7915fde680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MoE模型是一种混合专家模型的简写，它将传统Transformer模型中的每个前馈网络（FFN）层替换为MoE层，其中MoE层由两个核心部分组成：一个门控网络和若干数量的专家。MoE模型具有以下特点：1. 训练效率高，能够实现更高效的计算预训练；2. 与稠密模型相比，推理速度更快；3. 具有较好的泛化能力，但在微调阶段面临泛化能力不足的问题；4. 参数需求较大，推理过程中只使用其中一部分，需要将所有参数加载到内存中，因此对内存的需求较高。'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"简单总结MoE模型的特点\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36f68792-e3b0-4f43-80b0-4d168b9b188a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Shazeer 将 MoE 应用于自然语言处理（NLP）领域。'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Shazeer将 MoE 应用在了哪个应用领域？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c77ba28-afd4-46ee-a79f-04685478c473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ST-MoE 论文中显示了哪些令牌组被发送给了哪个专家的表格。根据文中所述，混合专家模型 (MoE) 中的令牌会根据专家的权重比例被分配，以保证所有专家接收到大致相等数量的训练样本，从而平衡专家之间的选择。此外，辅助损失可以被用来鼓励给予所有专家相同的重要性，以避免令牌总是被发送给少数几个受欢迎的专家。'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"MoE模型的训练过程中，如何保证令牌不会总是被发送给少数几个受欢迎的专家？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a528c839-7216-4c16-913c-3c4ea69f089a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据所提供的信息，MoE模型在训练时通过令牌路由和负载均衡机制来保证token的负载均衡。具体来说，该模型将令牌发送到拥有所需专家的节点，每个节点处理不同批次的训练样本。这种设计可以确保每个专家接收到不同数量的令牌，从而实现token的负载均衡。此外，该模型还采用了容量因子来提高模型性能，但这也意味着更高的通信成本和对保存激活值的显存的需求。在设备通信带宽有限的情况下，选择较小的容量因子可能是更佳的策略。'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"MoE模型训练时，怎么保证token的负载均衡？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a942a34a-e35a-4d37-9b23-1c029a2d2e25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GShard 保证 MoE 模型的负载均衡主要通过以下几个方面：\\n1. 随机路由：在 Top-2 设置中，我们始终选择排名最高的专家，但第二个专家是根据其权重比例随机选择的。\\n2. 专家容量：可以设定一个阈值，定义一个专家能处理多少令牌。如果两个专家的容量都达到上限，令牌就会溢出，并通过残差连接传递到下一层，或在某些情况下被完全丢弃。\\n3. 辅助损失：为了鼓励给予所有专家相同的重要性，引入了一个辅助损失，确保所有专家接收到大致相等数量的训练样本，从而平衡了专家之间的选择。\\n通过这些措施，GShard 能够实现 MoE 模型的负载均衡，从而提高训练效率。'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"GShard如何保证MoE模型的负载均衡？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11db4b19-d06d-426a-b950-5221297f50ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据所提供的信息，混合专家模型（MoE）的专家数量达到一定程度后，效率会显著降低。具体来说，当专家数量超过一定程度（约为1000-2000）时，训练和推理速度会明显变慢。因此，在选择专家数量时，需要权衡模型性能和计算资源的使用。'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"MoE的专家数量达到多少以后，效率就会显著降低？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aaf63bca-d75c-4249-9f88-f8f633ddd74d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这是一个比较复杂的问题，因为机器学习模型的效率受到很多因素的影响，包括数据集大小、模型结构、超参数选择等等。同时，专家数量也不是一个固定的值，而是需要根据具体问题进行调整。\\n\\n一般来说，当专家数量达到一定程度时，模型的效率可能会出现下降趋势。这是因为随着专家数量的增加，模型的复杂度也会增加，导致模型训练时间增加，同时过拟合的风险也会增加。但是，具体多少数量的专家会使得效率显著降低，需要根据具体问题和数据集情况进行实验和分析。\\n\\n另外，还有一些技巧可以帮助提高模型效率，例如正则化、早停、Dropout等。这些技巧可以在一定程度上减少过拟合现象，提高模型的泛化能力，同时也可以降低模型的复杂度，提高训练效率。'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatglm_model(\"MoE的专家数量达到多少以后，效率就会显著降低？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f970cab2-32b3-416c-b497-dae02450efc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据ST-MoE论文，微调稀疏混合专家模型（MoE）时，可以通过以下方法防止过拟合：\\n\\n1. 降低学习率和调大批量可以提升稀疏模型微调质量。\\n2. 尝试冻结所有非专家层的权重，实践中，这会导致性能大幅下降，但这符合预期，因为混合专家模型（MoE）层占据了网络的主要部分。可以尝试相反的方法：仅冻结MoE层的参数。实验结果显示，这种方法几乎与更新所有参数的效果相当。这种做法可以加速微调过程，并降低显存需求。\\n3. 考虑特别的微调超参数设置，例如，稀疏模型往往更适合使用较小的批量大小和较高的学习率，这样可以获得更好的训练效果。'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"在微调MoE这样的稀疏模型时，怎么防止出现过拟合？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4619ca89-0c53-46af-a761-3f87a67eefa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'将MoE模型的推理阶段更加高效的方法包括：\\n\\n1. 优化门控网络和专家的权重训练过程，以提高模型在微调阶段的泛化能力。\\n2. 采用更高效的计算方法，例如将稀疏混合专家模型 (SMoE) 蒸馏回具有更少实际参数但相似等价参数量的稠密模型。\\n3. 探索合并专家模型的技术及其对推理时间的影响。\\n4. 尝试对Mix'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"怎么将MoE模型的推理阶段更加高效？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52b3e27f-6fed-492f-985f-35c3932964ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'问题: 模型的并行有几种形式？\\n答案: 数据并行、模型并行、模型和数据并行、专家并行。'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"模型的并行有几种形式？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aaead7cf-7f50-4ea2-85a7-d807038eeb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'部署 MoE 模型有以下几种技术：\\n\\n1. 预先蒸馏 (Pre-distillation)：将 MoE 模型蒸馏回其对应的稠密模型，以保留稀疏性带来的性能提升，同时使得在推理中使用更小型的模型成为可能。\\n\\n2. 任务级别路由 (Task-level routing)：将整个句子或任务直接路由到一个专家，以提取出一个用于服务的子网络，有助于简化模型的结构。\\n\\n3. 专家网络聚合 (Expert network aggregation)：通过合并各个专家的权重，在推理时减少了所需的参数数量，在不显著牺牲性能的情况下降低模型的复杂度。'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"部署 MoE 模型有哪些技术?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f69fbd-4120-460a-87ce-115f45c4e0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87240bb2-f904-415f-acf0-4b90af15b786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "11bec224-dcab-46db-87eb-63bb10ad9d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "75e7102f-b99b-47bc-93b9-9360fcd200fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b5720-d341-4c3b-b768-5739fb859539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e91e01-c178-45ab-b3a1-a65e148accc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa53b4-3c34-462a-89f2-5dc91fa5250d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe95e5-a506-4aed-bbab-60982914a9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b04c2-e808-470d-b2fc-8005cba01497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5243026f-1408-433b-9c45-8cc727f957d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec36720-dc00-434b-9d58-3d33134b99e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c882a-1714-423e-a430-970b34eb38f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "053fc7df-0d25-44b1-92fc-f9abbaecbca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f0385-8341-4c7b-947e-f5021ad49292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b2741-ce7e-4a02-b160-ad9360f8e7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686fdb1-7715-4dd6-9dc0-c3d016d7c134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
