{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e2bbbc-d038-471b-8193-1572fb5f6647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade --quiet langchain langchain-community chromadb bs4 boto3\n",
    "# %pip install --upgrade --quiet pydantic\n",
    "# %pip install --upgrade --quiet sentence-transformers\n",
    "\n",
    "# %pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddb3d82b-7bcd-4378-8fe6-784519c8bf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade --quiet trulens_eval\n",
    "# %pip install --upgrade --quiet jinja2\n",
    "\n",
    "# %pip install --upgrade --quiet evaluate\n",
    "# %pip install --upgrade --quiet rouge_score\n",
    "# %pip install --upgrade --quiet bert_score\n",
    "%pip install --upgrade --quiet rouge-chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8824bc10-eb43-4f36-929b-9c7c6e6d5199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade ipywidgets\n",
    "# \"ipython>=8.12.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80401d49-eb2b-4ef2-9638-070fac6d995b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: trulens-eval\n",
      "Version: 0.28.2\n",
      "Summary: Library to systematically track and evaluate LLM based applications.\n",
      "Home-page: https://www.trulens.org\n",
      "Author: Truera Inc\n",
      "Author-email: all@truera.com\n",
      "License: MIT\n",
      "Location: /home/ubuntu/workshops/.conda/lib/python3.11/site-packages\n",
      "Requires: alembic, dill, frozendict, humanize, langchain, langchain-core, merkle-json, millify, munch, nest-asyncio, nltk, numpy, packaging, pip, psutil, pydantic, python-dotenv, requests, rich, sqlalchemy, streamlit, streamlit-aggrid, streamlit-extras, streamlit-pills, tqdm, typing-extensions, typing-inspect\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show trulens_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154308d2-f995-4b32-8e38-8830947691b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.1.16\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/ubuntu/workshops/.conda/lib/python3.11/site-packages\n",
      "Requires: aiohttp, dataclasses-json, jsonpatch, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: trulens-eval\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9078528-14f5-4f41-9489-ed1edc011f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "import json\n",
    "\n",
    "sagemaker_endpoint_name = 'chatglm3-6b-endpoint-2024-04-30-09-58-32-919'\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        parameters = model_kwargs\n",
    "        if 'temperature' not in parameters:\n",
    "            parameters['temperature'] = 0.01\n",
    "            parameters['max_new_tokens'] = 512\n",
    "        input = {\"inputs\": prompt, \"parameters\": parameters }\n",
    "        input_str = json.dumps(input).encode('utf-8')\n",
    "        return input_str\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"generated_text\"]\n",
    "\n",
    "chatglm_model = SagemakerEndpoint(\n",
    "    endpoint_name=sagemaker_endpoint_name, \n",
    "    region_name=\"us-east-1\", \n",
    "    content_handler=ContentHandler()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025a101e-3613-47a5-a223-8a77e3bd54f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amazon Bedrock是一个由亚马逊开发的软件开发工具,旨在帮助开发人员构建和部署应用程序。Bedrock是一个基于后端API的构建系统,它使用现代的软件构建工具和语言,如.NET、Java和Python等,来构建和打包应用程序。\n",
      "\n",
      "Bedrock的目标是提供一个可扩展、可靠和安全的构建系统,它具有以下特点:\n",
      "\n",
      "1. 可扩展性:Bedrock使用云技术来提供可扩展的构建能力,可以在需要时自动扩展或缩小构建资源。\n",
      "\n",
      "2. 可靠性:Bedrock使用自动化的工具和流程来确保构建过程的可靠性和一致性。\n",
      "\n",
      "3. 安全性:Bedrock使用安全工具和流程来保护应用程序代码和数据的安全性。\n",
      "\n",
      "Bedrock适用于需要构建和部署大型、复杂应用程序的开发团队。它可以帮助开发人员提高生产力、降低成本,并确保应用程序的可靠性和安全性。\n",
      "CPU times: user 26 ms, sys: 0 ns, total: 26 ms\n",
      "Wall time: 5.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = chatglm_model.invoke(\"Amazon Bedrock是什么?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02f337a-12dd-4f94-9885-631c48394eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='moka-ai/m3e-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c496afba-f571-4bbb-9c7a-705a2cd9a68d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "\n",
    "persist_directory=\"./chroma_500chunk/\"\n",
    "if (os.path.exists(persist_directory)):\n",
    "    vectorstore = Chroma(embedding_function=embeddings, persist_directory=persist_directory)\n",
    "else:\n",
    "    loader = WebBaseLoader(\"https://huggingface.co/blog/zh/moe\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    print('splits doc count:', len(splits))\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=persist_directory)\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e61bed4-97bf-484e-91c9-533890178454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "golden_set = [\n",
    "    {'query': 'MOE架构的模型有什么优势?',    'response': '相对来说，MOE模型有更快的训练速度和推理速度，并且它不需要将所有参数都加载到显存中，所以需要较小的显存。'},\n",
    "    {'query': '为什么MOE的训练和推理的速度更快?',    'response': '通过门控网络路由，每个token只选择一小部分专家网络计算，所以使用的显存较小，相对同等参数规模的模型，训练和推理速度也更快。而且通过并行不同的专家网络，可通过并行提高训练和推理效率。'},\n",
    "    {'query': '简单总结MoE模型的特点?',    'response': '与相同参数量的模型相比，训练和推理都更快，但是在训练和调优时，都有些额外的挑战。'},\n",
    "    {'query': 'Shazeer 将 MoE 应用在了哪个应用领域?',    'response': '主要集中在翻译领域。'},\n",
    "    {'query': 'MoE模型的训练过程中，如何保证令牌不会总是被发送给少数几个受欢迎的专家??',    'response': '混合专家模型通过以下两种机制，保证令牌的负载均衡，首先引入一个辅助损失，确保所有专家接收到的训练样本数大致相等；其次，再引入专家容量，定义一个专家能处理的令牌的阈值'},\n",
    "    {'query': 'MoE模型训练时，怎么保证token的负载均衡?',    'response': '混合专家模型通过以下两种机制，保证令牌的负载均衡，首先引入一个辅助损失，确保所有专家接收到的训练样本数大致相等；其次，再引入专家容量，定义一个专家能处理的令牌的阈值。'},\n",
    "    {'query': 'GShard如何保证MoE模型的负载均衡?',    'response': 'GShard的作者，为了保证令牌的负载均衡，除了引入辅助损失，确保所有专家接收到的训练样本数大致相等，还通过随机路由确保top-2设置中的专家的权重；再通过设置专家容量，定义一个专家能处理的令牌的阈值。'},\n",
    "    {'query': 'MoE的专家数量达到多少以后，效率就会显著降低?',    'response': '当专家数量达到 256 或 512 之后，效率的降低更为明显。'},\n",
    "    {'query': '怎么能够把MOE模型变得更小，让它更适合部署?',    'response': '为了使模型更适合部署，有以下几种技术，预蒸馏的实验、任务界别的路由、以及专家网络聚合。'},\n",
    "    {'query': '在微调MoE这样的稀疏模型时，怎么防止出现过拟合?',    'response': '可以使用使用辅助损失来避免过拟合，并设置较高的dropout 率，来优化模型性能。'},\n",
    "    {'query': '模型的并行有几种形式?',    'response': '数据并行，模型并行，模型和数据并行，专家并行。'},\n",
    "    {'query': '有哪些开源的MOE模型?',    'response': '有 Switch Transformers(Google)，NLLB MoE (Meta)，OpenMoE， Mixtral 8x7B (Mistral)'}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5abbac-b366-4e56-9472-d99b9402c752",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "了解了 MoE 的基本概念后，让我们进一步探索推动这类模型发展的研究。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t混合专家模型简史\n",
      "\t\n",
      "\n",
      "混合专家模型 (MoE) 的理念起源于 1991 年的论文 Adaptive Mixture of Local Experts。这个概念与集成学习方法相似，旨在为由多个单独网络组成的系统建立一个监管机制。在这种系统中，每个网络 (被称为“专家”) 处理训练样本的不同子集，专注于输入空间的特定区域。那么，如何选择哪个专家来处理特定的输入呢？这就是门控网络发挥作用的地方，它决定了分配给每个专家的权重。在训练过程中，这些专家和门控网络都同时接受训练，以优化它们的性能和决策能力。\n",
      "在 2010 至 2015 年间，两个独立的研究领域为混合专家模型 (MoE) 的后续发展做出了显著贡献:\n",
      "[Switch Transformers paper](https://arxiv.org/abs/2101.03961) 论文中的 MoE layer\n",
      "\n",
      "总结来说，在混合专家模型 (MoE) 中，我们将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个门控网络和若干数量的专家。\n",
      "尽管混合专家模型 (MoE) 提供了若干显著优势，例如更高效的预训练和与稠密模型相比更快的推理速度，但它们也伴随着一些挑战:\n"
     ]
    }
   ],
   "source": [
    "results = vectorstore.similarity_search(golden_set[0]['query'], k=2)\n",
    "for r in results:\n",
    "    print(r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c4293f8-c8cc-4ada-9540-ebf3bedeceed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦑 Tru initialized with db url sqlite:///default.sqlite .\n",
      "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "# initialize evaluation db\n",
    "tru = Tru()\n",
    "# tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c1984b-a058-424f-b17a-0002bf9cae2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trulens_eval.tru_custom_app import instrument\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"请基于以下的3引号里面的内容，简洁、准确的回答最后的问题，并使用中文。如果你无法从已知内容获取答案，就直接返回'根据已知信息无法回答该问题。'，不要编造答案。\n",
    "\n",
    "已知内容:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "问题: {question}\n",
    "答案:\"\"\"\n",
    "\n",
    "class RAG_App:\n",
    "    @instrument\n",
    "    def retrieve(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        results = vectorstore.similarity_search(query, k=5)\n",
    "        docs = [doc.page_content for doc in results]\n",
    "        return docs\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, query: str, context: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from context.\n",
    "        \"\"\"\n",
    "        context_str = \"\\n\\n\".join(context)\n",
    "        p = prompt_template.format(question=query, context=context_str)\n",
    "        return chatglm_model.invoke(p)\n",
    "\n",
    "    @instrument\n",
    "    def query(self, query: str) -> str:\n",
    "        context_slist = self.retrieve(query)\n",
    "        completion = self.generate_completion(query, context_slist)\n",
    "        return completion\n",
    "\n",
    "rag = RAG_App()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e21406c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Bedrock\n",
    "from typing import Dict, Optional, Sequence\n",
    "from trulens_eval.feedback import GroundTruthAgreement\n",
    "\n",
    "class Claude3_Bedrock(Bedrock):\n",
    "    def _create_chat_completion(\n",
    "        self,\n",
    "        prompt: Optional[str] = None,\n",
    "        messages: Optional[Sequence[Dict]] = None,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        assert self.endpoint is not None\n",
    "\n",
    "        import json\n",
    "        system_prompt = None\n",
    "        if messages:\n",
    "            if len(messages) == 1:\n",
    "                messages[0]['role'] = 'user'\n",
    "            elif messages[0]['role'] == 'system':\n",
    "                system_prompt = messages[0]['content']\n",
    "                messages.pop(0)\n",
    "        elif prompt:\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }]\n",
    "        else:\n",
    "            raise ValueError(\"Either 'messages' or 'prompt' must be supplied.\")\n",
    "        \n",
    "        body = {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 2048,\n",
    "                \"temperature\": 0.01,\n",
    "                \"messages\": messages\n",
    "        }\n",
    "        if system_prompt:\n",
    "            body['system'] = system_prompt\n",
    "        response = self.endpoint.client.invoke_model(body=json.dumps(body), modelId=self.model_id)\n",
    "        response_body = json.loads(response.get('body').read()).get('content')[0]['text']\n",
    "        return response_body\n",
    "    \n",
    "\n",
    "from typing import Dict, Tuple, Union\n",
    "from trulens_eval.utils.generated import re_0_10_rating\n",
    "import re, json\n",
    "\n",
    "def extract_between_tags(tag: str, string: str) -> list[str]:\n",
    "    ext_list = re.findall(f\"<{tag}>(.+?)</{tag}>\", string, re.DOTALL)\n",
    "    if ext_list:\n",
    "        return ext_list[0]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "class RAGGroundTruthAgreement(GroundTruthAgreement):\n",
    "    def agreement_measure_with_cot(\n",
    "        self, prompt: str, response: str\n",
    "    ) -> Union[float, Tuple[float, Dict[str, str]]]:\n",
    "        ground_truth_response = self._find_response(prompt)\n",
    "\n",
    "        AGREEMENT_SYSTEM_PROMPT = \"\"\"你的任务是根据给定的提示词和参考答案,评价一个AI生成的答案。具体来说:\n",
    "\n",
    "1. 首先,仔细阅读提示词,这是AI生成答案时需要遵守的要求：\n",
    "<prompt>%(prompt)s</prompt>\n",
    "\n",
    "2. 然后,查看参考答案，这是人类专家给出的理想答案：\n",
    "<reference>%(reference)s</reference> \n",
    "\n",
    "3. 最后,评估AI生成的答案,看它是否满足了提示词的要求,以及与参考答案有多大出入：\n",
    "<answer>%(answer)s</answer>\n",
    "\n",
    "在评估时,首先以 ‘score:’开头,给出一个0-10分的分数,10分表示完全满足要求且与参考答案一致,0分表示完全不符合要求。\n",
    "然后在 reason: 后写下你的评价理由和分析,解释AI生成答案的优缺点。\n",
    "\n",
    "因此,你的回复应该按照如下格式:\n",
    "\n",
    "score: 0-10分数值\n",
    "reason:\n",
    "...你对AI生成答案的评价理由...\n",
    "\n",
    "请谨记,你的评分和理由应该客观公正,既不过于苛刻,也不过于宽松。对AI生成的答案进行全面而中肯的评估。\n",
    "\"\"\"\n",
    "\n",
    "        agreement_txt = self.provider.endpoint.run_in_pace(\n",
    "            func=self.provider._create_chat_completion,\n",
    "            prompt=(AGREEMENT_SYSTEM_PROMPT % {'prompt': prompt, 'reference': ground_truth_response, 'answer': response})\n",
    "        )\n",
    "        score = re_0_10_rating(agreement_txt)\n",
    "        reason_index = agreement_txt.find('reason:')\n",
    "        if reason_index < 0:\n",
    "            reason_index = 0\n",
    "        reason = agreement_txt[reason_index:]\n",
    "        ret = score / 10, dict(\n",
    "            ground_truth_response=ground_truth_response,\n",
    "            reason=reason\n",
    "        )\n",
    "        return ret\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2d5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fclaude = Claude3_Bedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a47aa4-0fc8-4964-8eb8-631a082a70b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Groundedness, input source will be set to __record__.app.retrieve.rets.collect() .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input question will be set to __record__.app.retrieve.args.query .\n",
      "✅ In Context Relevance, input context will be set to __record__.app.retrieve.rets.collect() .\n",
      "✅ In Ground Truth, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Ground Truth, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Ground Truth, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Ground Truth, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Feedback, Select, Bedrock\n",
    "\n",
    "from trulens_eval.feedback import Groundedness, GroundTruthAgreement\n",
    "import numpy as np\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=fclaude)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(fclaude.relevance_with_cot_reasons, name = \"Answer Relevance\").on_input_output()\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(fclaude.qs_relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "    .on(Select.RecordCalls.retrieve.args.query)\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# Create a Feedback object for ground truth similarity\n",
    "old_ground_truth = GroundTruthAgreement(golden_set, provider = fclaude)\n",
    "f_old_groundtruth = (Feedback(old_ground_truth.agreement_measure, name = \"Ground Truth\")\n",
    "                 .on_input_output()\n",
    "                )\n",
    "\n",
    "# custom ground truth \n",
    "ground_truth = RAGGroundTruthAgreement(golden_set, provider = fclaude)\n",
    "# Call the agreement measure on the instruction and output\n",
    "f_groundtruth = (Feedback(ground_truth.agreement_measure_with_cot, name = \"Ground Truth\")\n",
    "                #  .on(Select.Record.calls[0].args.args[0])\n",
    "                 .on_input_output()\n",
    "                )\n",
    "# f_bert = Feedback(ground_truth.bert_score).on_input_output()\n",
    "# f_bleu = Feedback(ground_truth.bleu).on_input_output()\n",
    "# f_rouge = Feedback(ground_truth.rouge).on_input_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d5a56b0-e568-42be-b75b-f58a4ae315f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trulens_eval import TruCustomApp\n",
    "\n",
    "tru_rag_old = TruCustomApp(rag, app_id='ChatGLM_rag_old', feedbacks=[f_groundedness, f_old_groundtruth, f_qa_relevance, f_context_relevance])\n",
    "\n",
    "# with custom ground truth\n",
    "# tru_rag = TruCustomApp(rag, app_id='ChatGLM_rag', feedbacks=[f_groundedness, f_groundtruth, f_qa_relevance, f_context_relevance])\n",
    "\n",
    "# tru_rag_full = TruCustomApp(rag, app_id='ChatGLM_500Chunk_full', feedbacks=[f_groundtruth, f_bert, f_bleu, f_rouge])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b55414-1752-4759-aef0-1facd49b9ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "529a9310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514aa4064c984ff3b1187a39882bda3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tru_rag_old as recording:\n",
    "    rag.query(golden_set[8]['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c2fc775-5bef-4602-b8df-0fc235b67c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f470bcf2bd484eeba172f6ad272524d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572c39ed19314c79aae235af3a039136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfd7d93fd4842ac9a9fda5e19d4a787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c40501b3c0f4facad39acb4fb6b94b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdd1b0d6ace41be971ef27321df7e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae4962fb14f4ff5814d82800127c7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004ecb3b1fac46099c3208231d1ccfd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e36f0e23213411081a2bf7e22f9f3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f9bc5fd1884c838dc1ce01f2b926fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72963410889438cbc497b40e148537f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815eae9a27d04d4581c1528795dc0d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cee1745b47482591d2218744aacfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tru_rag_old as recording:\n",
    "    for question in golden_set:\n",
    "        rag.query(question['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e3ef238-29f9-4d7a-8be6-babe89dfe0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract5</th>\n",
       "      <td>0.868836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract4</th>\n",
       "      <td>0.866071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract6</th>\n",
       "      <td>0.859321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract</th>\n",
       "      <td>0.825000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_rag_old</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.878571</td>\n",
       "      <td>0.778571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.785714</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract3</th>\n",
       "      <td>0.797583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_rag</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract2</th>\n",
       "      <td>0.727786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Ground Truth  Answer Relevance  Context Relevance  \\\n",
       "app_id                                                                \n",
       "ChatGLM_extract5      0.868836               NaN                NaN   \n",
       "ChatGLM_extract4      0.866071               NaN                NaN   \n",
       "ChatGLM_extract6      0.859321               NaN                NaN   \n",
       "ChatGLM_extract       0.825000               NaN                NaN   \n",
       "ChatGLM_rag_old       0.800000          0.878571           0.778571   \n",
       "ChatGLM_extract3      0.797583               NaN                NaN   \n",
       "ChatGLM_rag           0.750000          0.866667           0.775000   \n",
       "ChatGLM_extract2      0.727786               NaN                NaN   \n",
       "ChatGLM_extract1           NaN               NaN                NaN   \n",
       "\n",
       "                  Groundedness   latency  total_cost  \n",
       "app_id                                                \n",
       "ChatGLM_extract5           NaN  3.000000         0.0  \n",
       "ChatGLM_extract4           NaN  3.000000         0.0  \n",
       "ChatGLM_extract6           NaN  3.000000         0.0  \n",
       "ChatGLM_extract            NaN  3.000000         0.0  \n",
       "ChatGLM_rag_old            1.0  2.785714         0.0  \n",
       "ChatGLM_extract3           NaN  3.000000         0.0  \n",
       "ChatGLM_rag                1.0  3.083333         0.0  \n",
       "ChatGLM_extract2           NaN  3.000000         0.0  \n",
       "ChatGLM_extract1           NaN  3.000000         0.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b2255-dd8a-4c14-b944-b015a0346989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tru_recorder as recording:\n",
    "#     for question in questions:\n",
    "#         rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd1d632f-2f5a-467a-9070-f8f1047b4f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6542e431044052b72452fb926ff9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "tru.run_dashboard()\n",
    "# tru.stop_dashboard() # stop if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5322e41-23f2-4ca4-96b3-be5d45199e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4565027",
   "metadata": {},
   "source": [
    "### 要点总结和信息提取等场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "602810f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_set_ext = [\n",
    "    {\n",
    "        \"query\": \"2023年8月3日，韩国京畿道城南市，22岁嫌疑人崔某驾驶汽车撞人后进入皇后百货商场持刀行凶，导致1人死亡，13人受伤。\",\n",
    "        \"response\": \"时间: ['2023年8月3日']\\n地点: ['韩国京畿道城南市']\\n人名: ['崔某']\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"2023年8月11日，中国第13次北冰洋科学考察的第一阶段任务在中北冰洋太平洋扇区完成，考察队领队王金辉宣布，完成了大气、海洋、生物群落和资源调查等多项作业项目。\",\n",
    "        \"response\": \"时间: ['2023年8月11日']\\n地点: ['中北冰洋太平洋扇区']\\n人名: ['王金辉']\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"2023年7月20日，国家防总副总指挥、水利部部长李国英表示，海河流域北三河地区将出现暴雨洪水过程，水利部积极部署相关防御工作。\",\n",
    "        \"response\": \"时间: ['2023年7月20日']\\n地点: ['海河流域北三河地区']\\n人名: ['李国英']\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"2023年8月8日，京津冀、东北等地受到“杜苏芮”台风引发的严重洪涝灾害，圆通速递公司董事长喻会蛟宣布捐赠1000万元并发挥快递网络和供应链优势进行救援。\",\n",
    "        \"response\": \"时间: ['2023年8月8日']\\n地点: ['京津冀、东北']\\n公司: ['圆通速递\\n人名: ['喻会蛟']\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3582233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Feedback, Select, Bedrock\n",
    "\n",
    "from trulens_eval.feedback import Groundedness, GroundTruthAgreement\n",
    "import numpy as np\n",
    "\n",
    "class ItemizedGroundTruthAgreement(GroundTruthAgreement):\n",
    "    def agreement_measure_with_cot(\n",
    "        self, prompt: str, response: str\n",
    "    ) -> Union[float, Tuple[float, Dict[str, str]]]:\n",
    "        ground_truth_response = self._find_response(prompt)\n",
    "\n",
    "        AGREEMENT_SYSTEM_PROMPT = \"\"\"你的任务是对参考答案和生成答案进行比较,并计算准确率、召回率和F值。\n",
    "\n",
    "这是参考答案:\n",
    "<expected_answer>\n",
    "%(reference)s\n",
    "</expected_answer>\n",
    "\n",
    "这是生成答案:\n",
    "<generated_answer>\n",
    "%(answer)s\n",
    "</generated_answer>\n",
    "\n",
    "首先,检查参考答案和生成答案的内容，得到两个集合。然后计算以下指标:\n",
    "\n",
    "准确率 = 生成答案中正确词数 / 生成答案总词数\n",
    "召回率 = 生成答案中正确词数 / 参考答案总词数\n",
    "F值 = 2 * (准确率 * 召回率) / (准确率 + 召回率)\n",
    "\n",
    "请在<reasoning>标签中写下你的计算过程,然后用JSON格式输出准确率、召回率和F值,格式如下:\n",
    "\n",
    "<reasoning>\n",
    "...你的计算过程...\n",
    "</reasoning>\n",
    "\n",
    "<result>\n",
    "{\n",
    "\"accuracy\": 准确率值,\n",
    "\"recall\": 召回率值,\n",
    "\"f_score\": F值\n",
    "}\n",
    "</result>\n",
    "\"\"\"\n",
    "        agreement_txt = self.provider.endpoint.run_in_pace(\n",
    "            func=self.provider._create_chat_completion,\n",
    "            prompt=(AGREEMENT_SYSTEM_PROMPT % {'reference': ground_truth_response, 'answer': response})\n",
    "        )\n",
    "        scores = extract_between_tags('result', agreement_txt)\n",
    "        scores = json.loads(scores)\n",
    "        \n",
    "        score = scores['f_score']\n",
    "        reason = extract_between_tags('reasoning', agreement_txt)\n",
    "        ret = score, dict(\n",
    "            ground_truth_response=ground_truth_response,\n",
    "            scores=scores,\n",
    "            reason=reason\n",
    "        )\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6b0bf935-10c9-441e-9dad-cd7064c5d181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function <function Extract_App.extract at 0xed418c5c28e0> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c31c180> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c31da80> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c31e340> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c5c13a0> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c53cc20> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c5c3420> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c31cc20> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c687ce0> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function RAG_App.query at 0xed418cf1c7c0> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c5c2520> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c31e520> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c5c3560> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c5c1580> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c53ede0> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c53c5e0> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function RAG_App.retrieve at 0xed418cf1c680> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c5c07c0> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function RAG_App.generate_completion at 0xed418cf1c720> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c53efc0> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function Extract_App.extract at 0xed418c31f7e0> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.Extract_App object at 0xed418c217050> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Ground Truth, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Ground Truth, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# Create a Feedback object for ground truth similarity\n",
    "itemized_ground_truth = ItemizedGroundTruthAgreement(golden_set_ext, provider = fclaude)\n",
    "f_itemized_groundtruth = (Feedback(itemized_ground_truth.agreement_measure_with_cot, name = \"Ground Truth\")\n",
    "                          .on_input_output()\n",
    "                          )\n",
    "\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "\n",
    "ext_prompt_template = \"\"\"从下面的上下文中提取出时间、地点、公司、人名等信息。\n",
    "\n",
    "上下文:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "答案按下面等格式返回：\n",
    "时间: [\"提取的时间\"]\n",
    "地点: [\"提取的地点\"]\n",
    "公司: [\"提取的公司名\"]\n",
    "人名: [\"提取的人名\"]\n",
    "\n",
    "如果没有人名、公司名等信息，这一项就返回空列表：\n",
    "时间: [\"提取的时间\"]\n",
    "地点: [\"提取的地点\"]\n",
    "公司: []\n",
    "人名: []\n",
    "\"\"\"\n",
    "\n",
    "class Extract_App:\n",
    "\n",
    "    @instrument\n",
    "    def extract(self, context: str) -> str:\n",
    "        p = ext_prompt_template.format(context=context)\n",
    "        return chatglm_model.invoke(p)\n",
    "\n",
    "extract = Extract_App()\n",
    "\n",
    "from trulens_eval import TruCustomApp\n",
    "tru_extract = TruCustomApp(extract, app_id='ChatGLM_extract6', feedbacks=[f_itemized_groundtruth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e5981eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023年8月3日，韩国京畿道城南市，22岁嫌疑人崔某驾驶汽车撞人后进入皇后百货商场持刀行凶，导致1人死亡，13人受伤。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "时间: [\"2023年8月3日\"]\n",
      "地点: [\"韩国京畿道城南市\"]\n",
      "公司: []\n",
      "人名: [\"崔某\"]\n",
      "2023年8月11日，中国第13次北冰洋科学考察的第一阶段任务在中北冰洋太平洋扇区完成，考察队领队王金辉宣布，完成了大气、海洋、生物群落和资源调查等多项作业项目。\n",
      "\n",
      "时间: 2023年8月11日\n",
      "地点: 中北冰洋太平洋扇区\n",
      "公司: 无\n",
      "人名: 王金辉\n",
      "2023年7月20日，国家防总副总指挥、水利部部长李国英表示，海河流域北三河地区将出现暴雨洪水过程，水利部积极部署相关防御工作。\n",
      "\n",
      "时间: [\"2023年7月20日\"]\n",
      "地点: [\"海河流域北三河地区\"]\n",
      "公司: [\"水利部\"]\n",
      "人名: [\"李国英\"]\n",
      "2023年8月8日，京津冀、东北等地受到“杜苏芮”台风引发的严重洪涝灾害，圆通速递公司董事长喻会蛟宣布捐赠1000万元并发挥快递网络和供应链优势进行救援。\n",
      "\n",
      "时间: 2023年8月8日\n",
      "地点: 京津冀、东北等地\n",
      "公司: 圆通速递公司\n",
      "人名: 喻会蛟\n"
     ]
    }
   ],
   "source": [
    "for qq in golden_set_ext:\n",
    "    print(qq['query'])\n",
    "    print(extract.extract(qq['query']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fec5a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tru_extract as recording:\n",
    "    for question in golden_set_ext:\n",
    "        extract.extract(question['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b3a83f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ChatGLM_rag</th>\n",
       "      <td>0.775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.868836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.866071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.859321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.797583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.727786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChatGLM_extract1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Context Relevance  Groundedness  Ground Truth  \\\n",
       "app_id                                                            \n",
       "ChatGLM_rag                   0.775           1.0      0.750000   \n",
       "ChatGLM_extract5                NaN           NaN      0.868836   \n",
       "ChatGLM_extract4                NaN           NaN      0.866071   \n",
       "ChatGLM_extract6                NaN           NaN      0.859321   \n",
       "ChatGLM_extract                 NaN           NaN      0.825000   \n",
       "ChatGLM_extract3                NaN           NaN      0.797583   \n",
       "ChatGLM_extract2                NaN           NaN      0.727786   \n",
       "ChatGLM_extract1                NaN           NaN           NaN   \n",
       "\n",
       "                  Answer Relevance   latency  total_cost  \n",
       "app_id                                                    \n",
       "ChatGLM_rag               0.866667  3.083333         0.0  \n",
       "ChatGLM_extract5               NaN  3.000000         0.0  \n",
       "ChatGLM_extract4               NaN  3.000000         0.0  \n",
       "ChatGLM_extract6               NaN  3.000000         0.0  \n",
       "ChatGLM_extract                NaN  3.000000         0.0  \n",
       "ChatGLM_extract3               NaN  3.000000         0.0  \n",
       "ChatGLM_extract2               NaN  3.000000         0.0  \n",
       "ChatGLM_extract1               NaN  3.000000         0.0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d44bd8-7db3-468e-b017-e59135d77082",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/workshops/.conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `BedrockChat` was deprecated in LangChain 0.0.34 and will be removed in 0.3. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import ChatBedrock`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import BedrockChat\n",
    "import boto3\n",
    "\n",
    "session = boto3.Session()\n",
    "bedrock_client = session.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "model_kwargs = {\n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0.01,\n",
    "    # \"top_k\": 250,\n",
    "    # \"top_p\": 1,\n",
    "}\n",
    "\n",
    "claude3_llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    client=bedrock_client,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834365b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d84744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bda02413-33ce-4c0a-97e8-b4a4ccf94e61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='星际穿越是一部非常出色的科幻电影,从多个方面都值得一看:\\n\\n1. 科学性强 - 电影中涉及到的相对论、黑洞、高维度等概念都有一定的科学依据,导演与著名物理学家科普作家合作,努力让科学内容尽可能贴近现有理论。\\n\\n2. 视觉效果震撼 - 太空旅行、异次元空间等场景的视觉呈现非常梦幻而宏大,给人深刻的视觉冲击。\\n\\n3. 剧情设置巧妙 - 人类面临资源枯竭的未来,科学家们孤注一掷寻找新家园的故事情节紧凑且富有张力。\\n\\n4. 探讨哲理命题 - 影片蕴含了对爱、家庭、牺牲、人性等命题的思考,给人以启迪。\\n\\n5. 演员阵容出色 - 马修·麦康纳、安妮·海瑟薇等主演都有精彩的演绎。\\n\\n总的来说,这部电影在视觉体验和思想内涵上都达到了很高的水准,是一部值得反复观赏的佳作。你看过这部影片吗?有何其他感想?', response_metadata={'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0', 'usage': {'prompt_tokens': 22, 'completion_tokens': 396, 'total_tokens': 418}}, id='run-8dbbbbe1-955d-4779-bcdc-4ecc687d0311-0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"星际穿越这部电影怎么样?\")\n",
    "]\n",
    "claude3_llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2a64845-a49c-48ca-8e15-9e1ab88333c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='星际穿越是一部非常出色的科幻电影,从多个方面都值得一看:\\n\\n1. 科学性强 - 电影中涉及到的相对论、黑洞、高维度等概念都有一定的科学依据,导演与著名物理学家科普作家合作,努力让科学内容尽可能贴近现有理论。\\n\\n2. 视觉效果震撼 - 太空旅行、异次元空间等场景的视觉呈现非常梦幻而宏大,给人深刻的视觉冲击。\\n\\n3. 剧情设置巧妙 - 人类为了生存不得不去寻找新家园的故事情节引人入胜,加上亲情的元素也很打动人心。\\n\\n4. 哲理性强 - 影片蕴含了对时间、爱、生命等哲学命题的思考,给人以启迪。\\n\\n5. 演员阵容强大 - 马修·麦康纳、安妮·海瑟薇等主演都演技出众。\\n\\n总的来说,这部电影在娱乐性与思考性之间取得了很好的平衡,是一部值得细细品味的佳作。你看过这部影片吗?有何其他感想?', response_metadata={'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0', 'usage': {'prompt_tokens': 22, 'completion_tokens': 381, 'total_tokens': 403}}, id='run-25cd6003-5bf3-4644-aa31-98bb0399394f-0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 与上面的调用一样，后面会使用message API调用\n",
    "claude3_llm.invoke(\"星际穿越这部电影怎么样?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb999383",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"你的任务是根据给定的提示词和参考答案,评价一个AI生成的答案。具体来说:\n",
    "\n",
    "1. 首先,仔细阅读提示词，这是AI生成答案时需要遵守的要求：\n",
    "<prompt>{$PROMPT}</prompt>\n",
    "\n",
    "2. 然后,查看参考答案，这是人类专家给出的理想答案：\n",
    "<reference>{$REFERENCE}</reference> \n",
    "\n",
    "3. 最后,评估AI生成的答案，看它是否满足了提示词的要求,以及与参考答案有多大出入：\n",
    "<answer>{$ANSWER}</answer>\n",
    "\n",
    "在评估时,首先以 ‘score:’开头，给出一个0-10分的分数,10分表示完全满足要求且与参考答案一致,0分表示完全不符合要求。\n",
    "然后在 justification: 后写下你的评价理由和分析,解释AI生成答案的优缺点。\n",
    "\n",
    "因此,你的回复应该按照如下格式:\n",
    "\n",
    "score: 0-10分数值\n",
    "justification:\n",
    "...你对AI生成答案的评价理由...\n",
    "\n",
    "请谨记,你的评分和理由应该客观公正,既不过于苛刻,也不过于宽松。对AI生成的答案进行全面而中肯的评估。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291c7543",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"谢谢您的提问! Bedrock可以利用知识库实现检索增强生成。\"\n",
    "answer = \"Bedrock与知识库相结合，实现自动化RAG解决方案，加速上市速度，提高成本效益。\"\n",
    "\n",
    "prompt = \"\"\"仅使用提供的上下文回答问题。如果不在上下文内容，就说你不知道，不要试图编造答案。答案尽量不要超过三个句子，并尽可能保持答案简洁。总是说“谢谢您的提问！”在答案的开头。\n",
    "上下文:\n",
    "借助 Amazon Bedrock，您可以使用知识库启用检索增强生成 (RAG) 工作流程，并利用 LLM 的推理功能构建上下文应用程序。\n",
    "RAG 是一种流行的技术，它将私有数据的使用与大型语言模型 (LLM) 相结合。\n",
    "Amazon Bedrock 与知识库相结合，通过自动化 RAG 解决方案并减少代理的构建时间，加快上市速度。 添加知识库还可以消除不断训练模型以利用私有数据的需要，从而提高成本效益\n",
    "。RAG 首先根据用户的查询从数据存储（最常见的是向量索引）检索相关文档。 然后，它使用语言模型通过考虑检索到的文档和原始查询来生成响应。 知识库服务会自动为您执行以下 RAG 设置和实施步骤3\n",
    "\n",
    "问题: 请简明扼要用30个汉字以内来回答Bedrock与知识库什么关系？\"\"\"\n",
    "\n",
    "eval_prompt = eval_prompt.replace('{$REFERENCE}', ref).replace('{$ANSWER}', answer).replace('{$PROMPT}', prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a747bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 8\n",
      "justification: \n",
      "AI生成的答案总体上符合提示词的要求,并且与参考答案的内容基本一致。答案简洁地概括了Bedrock与知识库的关系,即Bedrock可以利用知识库实现自动化的检索增强生成(RAG)解决方案,从而加快上市速度和提高成本效益。这与参考答案\"Bedrock可以利用知识库实现检索增强生成\"的核心内容相符。\n",
      "\n",
      "不过,AI生成答案略显冗长,未能完全遵循\"尽量不要超过三个句子\"的要求。此外,它也没有在开头说\"谢谢您的提问!\"。因此,虽然内容方面较为完整,但在遵循提示词的细节要求上还有一些欠缺,所以给予8分较为合理。\n",
      "\n",
      "总的来说,AI生成的答案质量较高,能够准确回答问题,只是在一些细节方面与提示词要求有一些出入。\n"
     ]
    }
   ],
   "source": [
    "eval_result = claude3_llm.invoke(eval_prompt)\n",
    "print(eval_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7d5e794-9438-409e-bcbe-29703dcae450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d92ad79c-1c77-429b-9dec-f3831f92fc41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据已知信息，混合专家模型（MoE）的优势包括：1. 更高效的预训练，2. 与稠密模型相比，推理速度更快。'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.query(\"MOE有什么优势?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c34b261d-fb6c-40c4-99b6-374ba17a2ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'根据 Switch Transformers 论文中的描述，混合专家模型 (MoE) 通过将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个门控网络和若干数量的专家。这种替换可以提高训练和推理速度。具体来说，由于 MoE 模型只使用其中的一部分参数，所以在推理过程中只需要加载部分参数到内存中，从而降低了内存需求。此外，由于 MoE 模型中的专家数量相对较少，因此训练和推理过程中计算量更小，从而提高了速度。'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"为什么MOE的训练和推理的速度更快?\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "587a7669-caa8-4a0a-9edf-ae7915fde680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MoE模型是一种混合专家模型的简写，它将传统Transformer模型中的每个前馈网络（FFN）层替换为MoE层，其中MoE层由两个核心部分组成：一个门控网络和若干数量的专家。MoE模型具有以下特点：1. 训练效率高，能够实现更高效的计算预训练；2. 与稠密模型相比，推理速度更快；3. 具有较好的泛化能力，但在微调阶段面临泛化能力不足的问题；4. 参数需求较大，推理过程中只使用其中一部分，需要将所有参数加载到内存中，因此对内存的需求较高。'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"简单总结MoE模型的特点\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36f68792-e3b0-4f43-80b0-4d168b9b188a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Shazeer 将 MoE 应用于自然语言处理（NLP）领域。'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Shazeer将 MoE 应用在了哪个应用领域？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c77ba28-afd4-46ee-a79f-04685478c473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ST-MoE 论文中显示了哪些令牌组被发送给了哪个专家的表格。根据文中所述，混合专家模型 (MoE) 中的令牌会根据专家的权重比例被分配，以保证所有专家接收到大致相等数量的训练样本，从而平衡专家之间的选择。此外，辅助损失可以被用来鼓励给予所有专家相同的重要性，以避免令牌总是被发送给少数几个受欢迎的专家。'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"MoE模型的训练过程中，如何保证令牌不会总是被发送给少数几个受欢迎的专家？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a528c839-7216-4c16-913c-3c4ea69f089a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据所提供的信息，MoE模型在训练时通过令牌路由和负载均衡机制来保证token的负载均衡。具体来说，该模型将令牌发送到拥有所需专家的节点，每个节点处理不同批次的训练样本。这种设计可以确保每个专家接收到不同数量的令牌，从而实现token的负载均衡。此外，该模型还采用了容量因子来提高模型性能，但这也意味着更高的通信成本和对保存激活值的显存的需求。在设备通信带宽有限的情况下，选择较小的容量因子可能是更佳的策略。'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"MoE模型训练时，怎么保证token的负载均衡？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a942a34a-e35a-4d37-9b23-1c029a2d2e25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GShard 保证 MoE 模型的负载均衡主要通过以下几个方面：\\n1. 随机路由：在 Top-2 设置中，我们始终选择排名最高的专家，但第二个专家是根据其权重比例随机选择的。\\n2. 专家容量：可以设定一个阈值，定义一个专家能处理多少令牌。如果两个专家的容量都达到上限，令牌就会溢出，并通过残差连接传递到下一层，或在某些情况下被完全丢弃。\\n3. 辅助损失：为了鼓励给予所有专家相同的重要性，引入了一个辅助损失，确保所有专家接收到大致相等数量的训练样本，从而平衡了专家之间的选择。\\n通过这些措施，GShard 能够实现 MoE 模型的负载均衡，从而提高训练效率。'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"GShard如何保证MoE模型的负载均衡？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11db4b19-d06d-426a-b950-5221297f50ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据所提供的信息，混合专家模型（MoE）的专家数量达到一定程度后，效率会显著降低。具体来说，当专家数量超过一定程度（约为1000-2000）时，训练和推理速度会明显变慢。因此，在选择专家数量时，需要权衡模型性能和计算资源的使用。'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"MoE的专家数量达到多少以后，效率就会显著降低？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aaf63bca-d75c-4249-9f88-f8f633ddd74d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这是一个比较复杂的问题，因为机器学习模型的效率受到很多因素的影响，包括数据集大小、模型结构、超参数选择等等。同时，专家数量也不是一个固定的值，而是需要根据具体问题进行调整。\\n\\n一般来说，当专家数量达到一定程度时，模型的效率可能会出现下降趋势。这是因为随着专家数量的增加，模型的复杂度也会增加，导致模型训练时间增加，同时过拟合的风险也会增加。但是，具体多少数量的专家会使得效率显著降低，需要根据具体问题和数据集情况进行实验和分析。\\n\\n另外，还有一些技巧可以帮助提高模型效率，例如正则化、早停、Dropout等。这些技巧可以在一定程度上减少过拟合现象，提高模型的泛化能力，同时也可以降低模型的复杂度，提高训练效率。'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatglm_model(\"MoE的专家数量达到多少以后，效率就会显著降低？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f970cab2-32b3-416c-b497-dae02450efc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据ST-MoE论文，微调稀疏混合专家模型（MoE）时，可以通过以下方法防止过拟合：\\n\\n1. 降低学习率和调大批量可以提升稀疏模型微调质量。\\n2. 尝试冻结所有非专家层的权重，实践中，这会导致性能大幅下降，但这符合预期，因为混合专家模型（MoE）层占据了网络的主要部分。可以尝试相反的方法：仅冻结MoE层的参数。实验结果显示，这种方法几乎与更新所有参数的效果相当。这种做法可以加速微调过程，并降低显存需求。\\n3. 考虑特别的微调超参数设置，例如，稀疏模型往往更适合使用较小的批量大小和较高的学习率，这样可以获得更好的训练效果。'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"在微调MoE这样的稀疏模型时，怎么防止出现过拟合？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4619ca89-0c53-46af-a761-3f87a67eefa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'将MoE模型的推理阶段更加高效的方法包括：\\n\\n1. 优化门控网络和专家的权重训练过程，以提高模型在微调阶段的泛化能力。\\n2. 采用更高效的计算方法，例如将稀疏混合专家模型 (SMoE) 蒸馏回具有更少实际参数但相似等价参数量的稠密模型。\\n3. 探索合并专家模型的技术及其对推理时间的影响。\\n4. 尝试对Mix'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"怎么将MoE模型的推理阶段更加高效？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52b3e27f-6fed-492f-985f-35c3932964ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'问题: 模型的并行有几种形式？\\n答案: 数据并行、模型并行、模型和数据并行、专家并行。'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"模型的并行有几种形式？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aaead7cf-7f50-4ea2-85a7-d807038eeb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'部署 MoE 模型有以下几种技术：\\n\\n1. 预先蒸馏 (Pre-distillation)：将 MoE 模型蒸馏回其对应的稠密模型，以保留稀疏性带来的性能提升，同时使得在推理中使用更小型的模型成为可能。\\n\\n2. 任务级别路由 (Task-level routing)：将整个句子或任务直接路由到一个专家，以提取出一个用于服务的子网络，有助于简化模型的结构。\\n\\n3. 专家网络聚合 (Expert network aggregation)：通过合并各个专家的权重，在推理时减少了所需的参数数量，在不显著牺牲性能的情况下降低模型的复杂度。'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"部署 MoE 模型有哪些技术?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f69fbd-4120-460a-87ce-115f45c4e0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87240bb2-f904-415f-acf0-4b90af15b786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "11bec224-dcab-46db-87eb-63bb10ad9d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "75e7102f-b99b-47bc-93b9-9360fcd200fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b5720-d341-4c3b-b768-5739fb859539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e91e01-c178-45ab-b3a1-a65e148accc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa53b4-3c34-462a-89f2-5dc91fa5250d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe95e5-a506-4aed-bbab-60982914a9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b04c2-e808-470d-b2fc-8005cba01497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5243026f-1408-433b-9c45-8cc727f957d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec36720-dc00-434b-9d58-3d33134b99e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c882a-1714-423e-a430-970b34eb38f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "053fc7df-0d25-44b1-92fc-f9abbaecbca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f0385-8341-4c7b-947e-f5021ad49292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b2741-ce7e-4a02-b160-ad9360f8e7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686fdb1-7715-4dd6-9dc0-c3d016d7c134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
