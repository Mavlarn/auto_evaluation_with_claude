{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install --upgrade --quiet evaluate\n",
    "# %pip install --upgrade --quiet rouge_score\n",
    "# %pip install --upgrade --quiet bert_score\n",
    "# %pip install --upgrade --quiet rouge-chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = '相对来说，MOE模型有更快的训练速度和推理速度，并且它不需要将所有参数都加载到显存中，所以需要较小的显存。'\n",
    "p21 = 'MOE架构的模型一般具有更高效的训练速度和推理速度。'\n",
    "p22 = 'MOE架构的模型具有以下优势：\\n\\n1. 更高效的预训练：相较于传统的Transformer模型，MoE可以将每个前馈网络（FFN）层替换为MoE层，从而提高了模型在训练过程中的计算效率。\\n\\n2. 与稠密模型相比更快的推理速度：虽然MoE模型在预训练阶段可能具有更多的参数，但在推理过程中，它仅使用其中的一部分参数，这使得推理速度快于具有相同数量参数的稠密模型。\\n\\n3. 更强的泛化能力：与传统Transformer模型相比，MoE模型在微调阶段往往具有更好的泛化能力，不容易引发过拟合现象。\\n\\n然而，MoE模型也存在一些挑战，如高效训练和推理挑战。为了解决这些问题，研究人员提出了一系列创新技术，如FasterMoE和Megablocks。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "bert_scorer = BERTScorer(lang=\"zh\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3457, 0.3845]), tensor([0.6780, 0.1872]), tensor([0.4963, 0.2801]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P, R, F1 = scorer.score(cands, refs)\n",
    "bert_scorer.score(\n",
    "    [p1, p1],\n",
    "    [p21, p22]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0, 'rougeLsum': 1.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load('bleu')\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "rouge.compute(\n",
    "    predictions=[p21], \n",
    "    references=[p1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.5666666666666667,\n",
       " 'rouge2': 0.0,\n",
       " 'rougeL': 0.5666666666666667,\n",
       " 'rougeLsum': 0.5666666666666667}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.compute(\n",
    "    predictions=[p21, p22], \n",
    "    references=[p1, p1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_chinese import Rouge\n",
    "rouge_cn = Rouge()\n",
    "rouge_cn.get_scores([p21, p22], [p1, p1], avg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
